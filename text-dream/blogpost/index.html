<!--
@license
Copyright 2019 Google Inc.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<html>
<head>
<link rel="icon"
      type="image/png"
      href="./icon.png"/>
<link rel="stylesheet" href="style.css">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Language, trees, and geometry in neural networks">
<!-- <meta property="og:description" content="Language, trees, and geometry in neural networks"> -->
<!-- <meta property="og:url" content=""> -->
<meta property="og:image" content="header.png">
<meta name="twitter:card" content="summary_large_image">


<title>Deep Dreaming with BERT</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="../third_party/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<!-- <img src="header.png" style="width:1051px;height:280px"> -->


<body>
<div id='header'></div>
<div >
  <h1 style="margin-top:20px; margin-bottom:20px">What does BERT dream of, and how can we interpret
    it?</h1>
</div>





<span style="color:#999;font-style:italic">
A visual investigation of nightmares in sesame street, by Alex BÃ¤uerle, James Wexler,
and Martin Wattenberg.
</span>
<p>


<a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">DeepDream</a>, which has been intensively studied for neural networks for image data, aims at providing explanations for what individual neurons react to.
It changes the input to the network through gradient decent, with the goal of maximizing the activation value of neurons in these networks.
This can be thought of as similar to the initial training process, where through many iterations, we try to optimize a mathematical equation.
But instead of updating network parameters, DeepDream updates the input sample.
What this leads to is somewhat psychedelic but very interesting images, that can reveal to what kind of input these neurons react.

<p>
We wanted to know how <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> reacts to different inputs.
BERT is a model for natural language understanding.
It can be used for tasks such as sentiment analysis or next sentence prediction.
In another blogpost, (link emilys work) investigated this using corpus search.
They searched for sentences that highly activate certain neurons of the network.
As this provided some promising results, we wanted to see if we can get even better with dreaming.

<p>
When dreaming for images, the input to the model is gradually changed.
Looking at a single pixel in the input image, this could be a slow change from green to red.
The green value would gradually go down, while the red value would increase.
Language, however, is made of discrete structures.
Thus, there is no such gradual change to be made.
We can, for examlple, not slowly go from the word "green" to the word "red", as everything in between does not make sense.
We therefore have to use a little trick.
The trick that worked best for us (later you will see that best does not necessarily mean perfect in the realm of deep learning), is called the softmax-trick, and has already been used in a <a href="https://www.aclweb.org/anthology/W18-5437">paper from 2018 by Poerner et. al.</a>.
This trick allows us to soften the requirement of dicrete tokens, and instead input a linear combination of tokens into the model.
To assure that we do not end up with something crazy, it uses two mechanisms.
First, it constrains this linear combination to in its sum only consist of one token.
This is done by taking the softmax function over this smooth input distribution, i.e. $softmax(soft_token_distribution)$.
This, however, still leaves the problem that we can end up with any linear combination of tokens that sums to one, including such ones that are in between real tokens.
Therefore, we also make use of a temperature parameter.
Before applying the softmax function, we divide our token distribution vector by this temperature.
Dividing by large temperature values means that the softmax result will be even more smooth, whereas  dividing by very low temperature values results in a more spiky softmax function.
We want the model to be able to first explore different linear combinations of tokens, before having to decide on one token.
This can be done by slowly decreasing the temperature alongside the dreaming process.


<h2>What are BERTs dreams like?</h2>

Now that we know how DeepDream can be used in combination with text models, how do those results look like?
Well, very unpredictable.
Fore some neurons, it is possible to produce sentences that highly activate them (baseline always being the the results from corpus search).
However, other neurons to not give us these results at all.
There are different options of which words to be changed by the model.
We always keep the CLS and SEP tokens static.
CLS is a spectial classification token used by BERT for some downstream tasks, while SEP marks the end of a sentence.
When we allow the model to change these tokens as well, it seems to be confused and the approach is likely to fail.
In between those, one can change anything from one word to the whole sentence.
When changing single words, the model often finds one that leads to a high activation, but not always.
For whole sentences, the sucess rate is slightly reduced to something around 50% in our experience.

<p>
In the first place, this was surprising to us.
This method uses gradient decent and seemed to work for other models (<a href="https://www.aclweb.org/anthology/W18-5437">see Poerner et. al. 2018</a>).
However, BERT is a complex model, arguably much more complex than the models that have been previously investigated with this method.
Nonetheless, we wanted to sanity-check our approach, but more on that later.

<p>
So, why is BERT such a bad dreamer?
This is a question we try to answer <a href="https://ai.google/research/teams/brain/pair">PAIR</a>-style, by providing explainability tools to visually inspect those dreaming results.
We built visualization tools that allow users of Deep Draeming with text to investigate their dreaming processes.
We use them to reason about our approaches with BERT, but if you run into similar problems, feel free to use them with any model.
All these tools are publicly available on <a href="https://github.com/PAIR-code/interpretability/text-dream">GitHub</a>.

<h3>Visualizing the dreaming process</h3>

The first question we wanted to answer for these dreaming processes is how the input representation evolves over said process.
Here, it is interesting to look at when and how the model replaces certain words in the input.
At the same time, we want to see how the activation value of the neuron we are trying to maximize is evolving alongside the change of temperature which we use to force the model to pick real tokens.
Additionally, we compare the evolvement of this activation value to the activation we would get if we were to ignore the linear combination of tokens that we obtain using the softmax-trick, and insted snap our input to the top ranked tokens of the softmax function.
The visualization of this process can be seen in Figure 1.

<div class="image-div">
  <img src="sentence_dream.png" style="width:1100px;">
  <div class="caption" style="width:1100px">Figure 1. Visualization of the dreaming result over iterations. We skipped some iterations in the middle for simplicity. In the beginning, the model can use linear combinations of embeddings to get a high activation value (dark green). As the temperature (blue) loweres, the model has to stick more to real tokens and the activation and activation of actual token ids (light green) gets closer. In this run, the model succeeds in dreaming to a higher activation (0.0436 vs. 0.0570).</div>
</div>

<p>
As you see, this is not a valid english sentence anymore.
What was more concerning, however, is that while this is an example for a successful run, we were not able to consistently get back to the same or a higher activation than what we started with.
We always started with top 10 sentences from corpus search as a baseline, as this approach has a lot more freedom to change words and thus should be able to get to higher activations.
This is something we wanted to investigate further.

<p>
To look at these processes and what might go wrong here, we made the problem easier by having it only change one word in the input sentence.
We did this to check if it would still not always be able to reach high activation values, and while the probability of getting to a high activation value increased, it still did not work out all the time.
We had some ideas about that, which we wanted to investigate further:

<ol>
  <li>
  Temperature annealing seems to first, allow the model to pick any linear combination of tokens, before gradually reducing the number of tokens used for this linear combination.
  What if some tokens get removed from the combination but would highly activate the neuron in isolation?
  </li>
  <li>
  What if the model is so specialized, that its neuron is really interested in the token walk, but everything arounf that token is not activate the neuron very much? This would make the optimization problem with gradient decent very hard.
  </li>
</ol>

<h3>Visualizing the annealing process</h3>

To investigate our first idea for why BERT might have problems with getting good sleep, we want to see how the annealing progresses.
What is especially interesting in this case, is how the softmax distribution over the token space changes throughout the process.
We thus developed a visualization that helps us investigate this distribution for different iterations.
In Figure 2, you can see how this looks like.

<div class="image-div">
  <img src="top_words.png" style="width:700px;">
  <div class="caption" style="width:700px">
    Figure 2. Visualization of the softmax diftribution that is used for getting a linear combination of embeddings to feed into the model.
    We see that the model uses a linear combination of tokens at this step.
    Interestingly, the token "hands" has a very low value.
    If we went on, we could see that the model rules out more tokens and in the end is unable to swap back to hands to get a higher activation.
  </div>
</div>

<p>
It is also interesting to look at this in combination with the top activations for a specific word position.
We can do that by simply checking the activation for each word in the vovabulary, and visualizing that as well.
You can see that in Figure 3.

(Add figure)

Interestingly, .....

<h3>Visualizing similar embeddings</h3>

Another concern for why this method is not always leading to desired results is, that the optimization problem is hard.
A reason for this could be, that the models neuron are highly specialized on certrain tokens, while all the tokens around it might not activate the neuron much.
To get an idea about that, we developed another visualization that allows us to look at activations for tokens that are close to the token we know higly activates the neuron.



<h2>Conclusion</h2>

We do not know exactly why things play out the way they do when we try to let BERT dream.
But thanks to these interesting visualization approaches, we have some ideas for those questions.
Additionally, the dreaming process provided us with interesting insights of how BERT sees some inputs, for exapmple what the model puts its focus on for different layers.
This is in line with findings from other approaches, such as (ians work) and (bert explain paper).

<p>
  <span style="color:#999;font-style:italic">
    Many thanks to (think about whom to thank) Nina Poerner, and Ian Tenney for helpful feedback and discussions about this research.</i>
</span>

</body>

<script src="https://d3js.org/d3.v4.min.js"></script>
<script>var d3interval = d3.interval</script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
<script src='https://unpkg.com/d3-jetpack@2.0.20/build/d3-jetpack.js'></script>

<script src='script.js'></script>

</html>
